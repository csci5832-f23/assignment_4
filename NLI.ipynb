{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Inference With BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this homework, we will work on (NLI)[https://nlp.stanford.edu/projects/snli/].\n",
    "\n",
    "The task is, give two sentences: a premise and a hypothesis, to classify the relation between them. We have three classes to describe this relationship.\n",
    "\n",
    "1. Entailment: the hypothesis follows from the fact that the premise is true\n",
    "2. Contradiction: the hypothesis contradicts the fact that the premise is true\n",
    "3. Neutral: There is not relationship between premise and hypothesis\n",
    "\n",
    "See below for examples\n",
    "\n",
    "![snli_task](media/snli_task.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (4.34.1)\n",
      "Requirement already satisfied: datasets in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (2.14.6)\n",
      "Requirement already satisfied: tqdm in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (4.66.1)\n",
      "Requirement already satisfied: filelock in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: xxhash in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (3.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/rehan/anaconda3/envs/pya4/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports for most of the notebook\n",
    "import torch\n",
    "from transformers import BertModel\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Dict, List\n",
    "import random\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cpu\")\n",
    "# TODO: Uncomment the below line if you see True in the print statement\n",
    "# device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First let's load the Stanford NLI dataset from the huggingface datasets hub using the datasets package\n",
    "\n",
    "### Explore the dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes (num_samples, num_labels):\n",
      " {'test': (10000, 3), 'train': (550152, 3), 'validation': (10000, 3)}\n",
      "\n",
      "Example:\n",
      " {'premise': 'A person on a horse jumps over a broken down airplane.', 'hypothesis': 'A person is training his horse for a competition.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"snli\")\n",
    "print(\"Split sizes (num_samples, num_labels):\\n\", dataset.shape)\n",
    "print(\"\\nExample:\\n\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each example is a dictionary with the keys: (premise, hypothesis, label). \n",
    "\n",
    "#### Data Fields\n",
    " - premise: a string used to determine the truthfulness of the hypothesis\n",
    " - hypothesis: a string that may be true, false, or whose truth conditions may not be knowable when compared to the premise\n",
    " - label: an integer whose value may be either 0, indicating that the hypothesis entails the premise, 1, indicating that the premise and hypothesis neither entail nor contradict each other, or 2, indicating that the hypothesis contradicts the premise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation and Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c6812a223543308680ee10844e7ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Datapoints:   0%|          | 0/550152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a1f4391b9849f3ab0e781a57327a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Datapoints:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e336c8fdb0945078ea9d891ceef2735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading Datapoints:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_snli(train=10000, validation=1000, test=1000):\n",
    "    snli = load_dataset('snli')\n",
    "    train_dataset = get_even_datapoints(snli['train'], train)\n",
    "    validation_dataset = get_even_datapoints(snli['validation'], validation)\n",
    "    test_dataset = get_even_datapoints(snli['test'], test)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n",
    "\n",
    "def get_even_datapoints(datapoints, n):\n",
    "    random.seed(42)\n",
    "    dp_by_label = defaultdict(list)\n",
    "    for dp in tqdm(datapoints, desc='Reading Datapoints'):\n",
    "        dp_by_label[dp['label']].append(dp)\n",
    "\n",
    "    unique_labels = [0, 1, 2]\n",
    "\n",
    "    split = n//len(unique_labels)\n",
    "\n",
    "    result_datapoints = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        result_datapoints.extend(random.sample(dp_by_label[label], split))\n",
    "\n",
    "    return result_datapoints\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = get_snli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999 999 999\n",
      "Counter({0: 3333, 1: 3333, 2: 3333})\n",
      "Counter({0: 333, 1: 333, 2: 333})\n",
      "Counter({0: 333, 1: 333, 2: 333})\n"
     ]
    }
   ],
   "source": [
    "## sub set stats\n",
    "from collections import Counter\n",
    "\n",
    "# num sample stats\n",
    "print(len(train_dataset), len(validation_dataset), len(test_dataset))\n",
    "\n",
    "# label distribution\n",
    "print(Counter([t['label'] for t in train_dataset]))\n",
    "print(Counter([t['label'] for t in validation_dataset]))\n",
    "print(Counter([t['label'] for t in test_dataset]))\n",
    "\n",
    "# We have a perfectly balanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want a function to load samples from the huggingface dataset so that they can be batched and encoded for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's reimplement our tokenizer using the huggingface tokenizer.\n",
    "\n",
    "### Notice that our __call__ method (the one called when we call an instance of our class) takes both a premise batch and a hypothesis batch.\n",
    "### The HuggingFace BERT tokenizer knows to join these with the special sentence seperator token between them. We let HuggingFace do most of the work here for making batches of tokenized and encoded sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2023,  2003,  1996,  2034, 18458,   102,  2023,  2003,  2034,\n",
      "         10744,   102,     0],\n",
      "        [  101,  2023,  2003,  1996,  2117, 18458,   102,  2023,  2003,  1996,\n",
      "          2117, 10744,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[CLS] this is the first premise [SEP] this is first hypothesis [SEP] [PAD]',\n",
       " '[CLS] this is the second premise [SEP] this is the second hypothesis [SEP]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nothing to do for this class!\n",
    "\n",
    "class BatchTokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='prajjwal1/bert-small'):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<P>\".\n",
    "        \"\"\"\n",
    "        self.hf_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def get_sep_token(self,):\n",
    "        return self.hf_tokenizer.sep_token\n",
    "    \n",
    "    def __call__(self, prem_batch: List[str], hyp_batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Uses the huggingface tokenizer to tokenize and pad a batch.\n",
    "\n",
    "        We return a dictionary of tensors per the huggingface model specification.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            Dict: The dictionary of token specifications provided by HuggingFace\n",
    "        \"\"\"\n",
    "        # The HF tokenizer will PAD for us, and additionally combine \n",
    "        # The two sentences deimited by the [SEP] token.\n",
    "        enc = self.hf_tokenizer(\n",
    "            prem_batch,\n",
    "            hyp_batch,\n",
    "            padding=True,\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return enc\n",
    "    \n",
    "\n",
    "# HERE IS AN EXAMPLE OF HOW TO USE THE BATCH TOKENIZER\n",
    "tokenizer = BatchTokenizer()\n",
    "x = tokenizer(*[[\"this is the first premise\", \"This is the second premise\"], [\"This is first hypothesis\", \"This is the second hypothesis\"]])\n",
    "print(x)\n",
    "tokenizer.hf_tokenizer.batch_decode(x[\"input_ids\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can batch the train, validation, and test data, and then run it through the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_pairwise_input(dataset: List[Dict]) -> (List[str], List[str], List[int]):\n",
    "    \"\"\"\n",
    "    TODO: group all premises and corresponding hypotheses and labels of the datapoints\n",
    "    a datapoint as seen earlier is a dict of premis, hypothesis and label\n",
    "    \"\"\"\n",
    "    premises = []\n",
    "    hypothesis = []\n",
    "    labels = []\n",
    "    for x in dataset:\n",
    "        premises.append(x['premise'])\n",
    "        hypothesis.append(x['hypothesis'])\n",
    "        labels.append(x['label'])\n",
    "    \n",
    "    return premises, hypothesis, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_premises, train_hypotheses, train_labels = generate_pairwise_input(train_dataset)\n",
    "validation_premises, validation_hypotheses, validation_labels = generate_pairwise_input(validation_dataset)\n",
    "test_premises, test_hypotheses, test_labels = generate_pairwise_input(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def chunk_multi(lst1, lst2, n):\n",
    "    for i in range(0, len(lst1), n):\n",
    "        yield lst1[i: i + n], lst2[i: i + n]\n",
    "        \n",
    "batch_size = 16\n",
    "        \n",
    "# Notice that since we use huggingface, we tokenize and\n",
    "# encode in all at once!\n",
    "tokenizer = BatchTokenizer()\n",
    "train_input_batches = [b for b in chunk_multi(train_premises, train_hypotheses, batch_size)]\n",
    "# Tokenize + encode\n",
    "train_input_batches = [tokenizer(*batch) for batch in train_input_batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's batch the labels, ensuring we get them in the same order as the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])\n",
    "\n",
    "\n",
    "train_label_batches = [b for b in chunk(train_labels, batch_size)]\n",
    "train_label_batches = [encode_labels(batch) for batch in train_label_batches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we implement the model. Notice the TODO and the optional TODO (read why you may want to do this one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NLIClassifier(torch.nn.Module):\n",
    "    def __init__(self, output_size: int, hidden_size: int, model_name='prajjwal1/bert-small'):\n",
    "        super().__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Initialize BERT, which we use instead of a single embedding layer.\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        \n",
    "        # TODO [OPTIONAL]: Updating all BERT parameters can be slow and memory intensive. \n",
    "        # Freeze them if training is too slow. Notice that the learning\n",
    "        # rate should probably be smaller in this case.\n",
    "        # Uncommenting out the below 2 lines means only our classification layer will be updated.\n",
    "        \n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        self.bert_hidden_dimension = self.bert.config.hidden_size\n",
    "        \n",
    "        # TODO: Add an extra hidden layer in the classifier, projecting\n",
    "        #      from the BERT hidden dimension to hidden size. Hint: torch.nn.Linear()\n",
    "        \n",
    "        self.hidden_layer = None\n",
    "        \n",
    "        # TODO: Add a relu nonlinearity to be used in the forward method\n",
    "        #      https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html\n",
    "        \n",
    "        self.relu = None\n",
    "        \n",
    "        self.classifier = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: Dict\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols BERT.\n",
    "            Then, get CLS represenation.\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: CLS token embedding\n",
    "        \"\"\"\n",
    "        # First we get the contextualized embedding for each input symbol\n",
    "        # We no longer need an LSTM, since BERT encodes context and \n",
    "        # gives us a single vector describing the sequence in the form of the [CLS] token.\n",
    "        encoded_sequence = self.bert(**symbols)\n",
    "        # TODO: Get the [CLS] token\n",
    "        #      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\n",
    "        #      and check the returns for the forward method.\n",
    "        # We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\n",
    "        # print(encoded_sequence.last_hidden_state.shape)\n",
    "        # Return only the first token's embedding from the last_hidden_state. Hint: using list slices\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: Dict,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            symbols (Dict): The Dict of token specifications provided by the HuggingFace tokenizer\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: _description_\n",
    "        \"\"\"\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.hidden_layer(encoded_sents)\n",
    "        output = self.relu(output)\n",
    "        output = self.classifier(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For making predictions at test time\n",
    "def predict(model: torch.nn.Module, sents: torch.Tensor) -> List:\n",
    "    logits = model(sents.to(device))\n",
    "    return list(torch.Tensor.cpu(torch.argmax(logits, axis=2).squeeze()).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics: Macro F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sum as t_sum\n",
    "from numpy import logical_and\n",
    "\n",
    "\n",
    "def precision(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Precision is True Positives / All Positives Predictions\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(pred_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def recall(predicted_labels, true_labels, which_label=1):\n",
    "    \"\"\"\n",
    "    Recall is True Positives / All Positive Labels\n",
    "    \"\"\"\n",
    "    pred_which = np.array([pred == which_label for pred in predicted_labels])\n",
    "    true_which = np.array([lab == which_label for lab in true_labels])\n",
    "    denominator = t_sum(true_which)\n",
    "    if denominator:\n",
    "        return t_sum(logical_and(pred_which, true_which))/denominator\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def f1_score(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    which_label: int\n",
    "):\n",
    "    \"\"\"\n",
    "    F1 score is the harmonic mean of precision and recall\n",
    "    \"\"\"\n",
    "    P = precision(predicted_labels, true_labels, which_label=which_label)\n",
    "    R = recall(predicted_labels, true_labels, which_label=which_label)\n",
    "    \n",
    "    if P and R:\n",
    "        return 2*P*R/(P+R)\n",
    "    else:\n",
    "        return 0.\n",
    "\n",
    "\n",
    "def macro_f1(\n",
    "    predicted_labels: List[int],\n",
    "    true_labels: List[int],\n",
    "    possible_labels: List[int],\n",
    "    label_map=None\n",
    "):\n",
    "    converted_prediction = [label_map[int(x)] for x in predicted_labels] if label_map else predicted_labels\n",
    "    scores = [f1_score(converted_prediction, true_labels, l) for l in possible_labels]\n",
    "    # Macro, so we take the uniform avg.\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features.to(device)).squeeze(1)\n",
    "            loss = loss_func(preds, labels.to(device))\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating dev...\")\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "            pred = predict(model, sents)\n",
    "            all_preds.extend(pred)\n",
    "            all_labels.extend(list(labels.cpu().numpy()))\n",
    "\n",
    "        dev_f1 = macro_f1_score(all_preds, all_labels)\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e532c48c8f472288b6f03b34b96bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m validation_batch_labels \u001b[38;5;241m=\u001b[39m [b \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m chunk(validation_labels, batch_size)]\n\u001b[1;32m     20\u001b[0m validation_batch_labels \u001b[38;5;241m=\u001b[39m [encode_labels(batch) \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m validation_batch_labels]\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_input_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_label_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_input_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_batch_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(num_epochs, train_features, train_labels, dev_sents, dev_labels, optimizer, model)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(batches):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# Empty the dynamic computation graph\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 19\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_func(preds, labels\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Backpropogate the loss through our model\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pya4/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pya4/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m, in \u001b[0;36mNLIClassifier.forward\u001b[0;34m(self, symbols)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     60\u001b[0m     symbols: Dict,\n\u001b[1;32m     61\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"_summary_\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m        torch.Tensor: _description_\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     encoded_sents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     71\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layer(encoded_sents)\n\u001b[1;32m     72\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(output)\n",
      "Cell \u001b[0;32mIn[16], line 56\u001b[0m, in \u001b[0;36mNLIClassifier.encode_text\u001b[0;34m(self, symbols)\u001b[0m\n\u001b[1;32m     49\u001b[0m encoded_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msymbols)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# TODO: Get the [CLS] token\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m#      The BertModel output. See here: https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#      and check the returns for the forward method.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# We want to return a tensor of the form batch_size x 1 x bert_hidden_dimension\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# print(encoded_sequence.last_hidden_state.shape)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Return only the first token's embedding from the last_hidden_state. Hint: using list slices\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# You can increase epochs if need be\n",
    "epochs = 20\n",
    "\n",
    "# TODO: Find a good learning rate and hidden size\n",
    "LR = 0.0\n",
    "hidden_size = 10\n",
    "\n",
    "possible_labels = set(train_labels)\n",
    "model = NLIClassifier(output_size=len(possible_labels), hidden_size=hidden_size)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), LR)\n",
    "\n",
    "batch_tokenizer = BatchTokenizer()\n",
    "\n",
    "validation_input_batches = [b for b in chunk_multi(validation_premises, validation_hypotheses, batch_size)]\n",
    "\n",
    "# Tokenize + encode\n",
    "validation_input_batches = [batch_tokenizer(*batch) for batch in validation_input_batches]\n",
    "validation_batch_labels = [b for b in chunk(validation_labels, batch_size)]\n",
    "validation_batch_labels = [encode_labels(batch) for batch in validation_batch_labels]\n",
    "\n",
    "training_loop(\n",
    "    epochs,\n",
    "    train_input_batches,\n",
    "    train_label_batches,\n",
    "    validation_input_batches,\n",
    "    validation_batch_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a final macro F1 on the test set.\n",
    "# You should be able to mimic what we did with the validaiton set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Pretrained Model\n",
    "\n",
    "Huggingface also hosts models which users have already trained on SNLI -- we can load them here and evaluate their performance on the validation and test set. \n",
    "\n",
    "These models include the BertModel which we were using before, but also the trained weights for the classifier layer. Because of this, we'll use the standard HuggingFace classification model instead of the classifier we used above, and modify the training and prediction functions to handle this correctly.\n",
    "\n",
    "Try and find the differences between the training loop. One addition is the new usage of \"label_map\". Why may this be necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def class_predict(model, sents):\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        logits = model(**sents.to(device)).logits\n",
    "        predictions = torch.argmax(logits, axis=1)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "def prediction_loop(model, dev_sents, dev_labels, label_map=None):\n",
    "    print(\"Evaluating...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "        pred = class_predict(model, sents).cpu()\n",
    "        all_preds.extend(pred)\n",
    "        all_labels.extend(list(labels.cpu().numpy()))\n",
    "\n",
    "    dev_f1 = macro_f1(all_preds, all_labels, possible_labels=set(all_labels), label_map = label_map)\n",
    "    print(f\"F1 {dev_f1}\")\n",
    "\n",
    "def class_training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_sents,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "    label_map=None\n",
    "):\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    for i in range(num_epochs):\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(**features.to(device)).logits\n",
    "            # preds = torch.argmax(logits, axis=1)\n",
    "            loss = loss_func(logits, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "    print(\"Evaluating dev...\")\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for sents, labels in tqdm(zip(dev_sents, dev_labels), total=len(dev_sents)):\n",
    "        pred = predict(model, sents).cpu()\n",
    "        all_preds.extend(pred)\n",
    "        all_labels.extend(list(labels.numpy()))\n",
    "\n",
    "    all_preds\n",
    "    dev_f1 = macro_f1(all_preds, all_labels, possible_labels=set(all_labels), label_map = label_map)\n",
    "    print(f\"Dev F1 {dev_f1}\")\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load a model and re-tokenize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Get the label_map\n",
    "## For the snli dataset  0=Entailment, 1=Neutral, 2=Contradiction\n",
    "label_map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8df0d8c10f34641b9500fd8d03b8910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 0.19005567964129075\n"
     ]
    }
   ],
   "source": [
    "from transformers import  BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "\n",
    "model_name = 'textattack/bert-base-uncased-snli'\n",
    "tokenizer_model_name = 'textattack/bert-base-uncased-snli' # This is sometimes different from model_name, but should normally be the same\n",
    "\n",
    "model =  BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "batch_tokenizer = BatchTokenizer(model_name = tokenizer_model_name)\n",
    "\n",
    "validation_input_batches = [b for b in chunk_multi(validation_premises, validation_hypotheses, batch_size)]\n",
    "\n",
    "# Tokenize + encode\n",
    "validation_input_batches = [batch_tokenizer(*batch) for batch in validation_input_batches]\n",
    "validation_batch_labels = [b for b in chunk(validation_labels, batch_size)]\n",
    "validation_batch_labels = [encode_labels(batch) for batch in validation_batch_labels]\n",
    "\n",
    "prediction_loop(model, validation_input_batches, validation_batch_labels, label_map=label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this section, find 2 other BERT-based models which have been trained on natural language inference and evaluate them on the dev set. Note the scores for each model and any high-level differences you can find between the models (architecture, sizes, training data, etc. )\n",
    "\n",
    "If you don't have access to a GPU, inference may be slow, particularly for larger models. In this case, take a sample of the validation set; the size should be large enough such that all labels are covered, and a score will still be meaningful, but also so that inference doesn't take more than 3-5 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Written Assignment\n",
    "\n",
    "### 1. Describe the task and what capability is required to solve it.\n",
    "\n",
    "### 2. How does the method of encoding sequences of words in our model differ here, compared to the word embeddings in HW 4. What is different? Why benefit does this method have?\n",
    "\n",
    "### 3. Discuss your results. Did you do any hyperparameter tuning? Did the model solve the task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
